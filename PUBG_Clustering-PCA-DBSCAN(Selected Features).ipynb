{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PUBG_Logo](assets/PUBG_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* Employ Prinicpal Components Analyis using selected features on dataset.\n",
    "* Employ Density-based spatial clustering of applications with noise (DBSCAN) algorithm on dataset\n",
    "* Discuss pertinent results.\n",
    "\n",
    "## Background Information\n",
    "* Playerunknown's Battleground (PUBG) is a video game, which set the standard for preceding games in the Battle Royale Genre. The main goal is to SURVIVE at all costs.\n",
    "\n",
    "## Process:\n",
    "* Exploratory Data Analysis conducted utilizing various python packages (Numpy, Matplotlib, Pandas, and Plotly).\n",
    "* Principal Components Analysis (Sci-Kit Learn)\n",
    "* DBSCAN (Sci-Kit Learn)\n",
    "\n",
    "\n",
    "## Table of Contents:\n",
    "* Part I: Exploratory Data Analysis\n",
    "    * EDA\n",
    "* Part II: PCA / DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by reading in the CSV file containing the data, and examining the data contents such as the number of features and rows. It seems there are 152 column entries (features) and 87898 row entries (number of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- Pandas Dataframe\n",
    "## Read in CSV\n",
    "orig = pd.read_csv('data/PUBG_Player_Statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us remove and combine features, which do not pertain to our goal of clustering solo player behavior. \n",
    "\n",
    "Remove:\n",
    "* player_name\n",
    "* tracker_id\n",
    "* duo\n",
    "* squad\n",
    "\n",
    "Add:\n",
    "* Total Distance\n",
    "\n",
    "This can be achieved by removing all columns after the 52nd. Also, create a new feature that combines the walking and riding distance.\n",
    "\n",
    "Also, we will reduce the variance in the data by removing players with less than the mean number of rounds in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Preprocessing\n",
    "## Create a copy of the dataframe\n",
    "df = orig.copy()\n",
    "cols = np.arange(52, 152, 1)\n",
    "\n",
    "# Drop entries if they have null values\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "## Drop columns after the 52nd index\n",
    "df.drop(df.columns[cols], axis = 1, inplace = True)\n",
    "\n",
    "## Drop player_name and tracker id\n",
    "df.drop(df.columns[[0, 1]], axis = 1, inplace = True)\n",
    "\n",
    "## Drop Knockout and Revives\n",
    "df.drop(df.columns[[49]], axis = 1, inplace = True)\n",
    "df.drop(columns = ['solo_Revives'], inplace = True)\n",
    "\n",
    "## Drop the string solo from all strings\n",
    "df.rename(columns = lambda x: x.lstrip('solo_').rstrip(''), inplace = True)\n",
    "\n",
    "## Combine a few columns \n",
    "df['TotalDistance'] = df['WalkDistance'] + df['RideDistance']\n",
    "df['AvgTotalDistance'] = df['AvgWalkDistance'] + df['AvgRideDistance']\n",
    "\n",
    "# Remove Outliers\n",
    "df = df.drop(df[df['RoundsPlayed'] < df['RoundsPlayed'].mean()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into three sets: train, dev, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test set using Sci-Kit Learn\n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 10)\n",
    "dev, test = train_test_split(test, test_size = 0.2, random_state = 10)\n",
    "data = train\n",
    "\n",
    "print(\"The number of training samples is\", len(train))\n",
    "print(\"The number of development samples is\", len(dev))\n",
    "print(\"The number of testing samples is\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important we go through the final output to make sure that are data preprocessing is complete. And it looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', 52):\n",
    "    print(data.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only factors above which have a positive correlation to average survival time are Average Total Distance, Win Ratio, and Top 10 Ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedure: \n",
    "* 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering in 3D (Selected Few Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the following features because of experts and my domain experience playing PUBG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select four features\n",
    "train_data = train.loc[:,['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]\n",
    "dev_data = dev.loc[:, ['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]\n",
    "test_data = test.loc[:, ['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is utilized to make sure all features are normalized and have similar orders of magnitude. This is important because our clustering algorithms look into calculating the distance between points. In our case, we employed a zero-mean and unit-variance scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data (Normaliz)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(train_data)\n",
    "X_dev_std = scaler.transform(dev_data)\n",
    "X_test_std = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Principal Component Analysis is a dimensional reduction technique that preserves the essence of the original data [1]. Having multi-dimensional data (many features), it is very important in identifying which features are important. PCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along with them.\n",
    "\n",
    "\n",
    "#### Algorithm \n",
    "1. Calculate the covariance matrix X of data points.\n",
    "2. Calculate eigenvectors and corresponding eigen values.\n",
    "3. Sort the eigenvectors according to their eigen values in decreasing order.\n",
    "4. Choose first k eigen vectors and that will be the new k dimensions.\n",
    "5. Transform the original n dimensional data points into k dimensions.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "PCA has one parameter:\n",
    "* Number of components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Number of Components\n",
    "\n",
    "Now, we'll want to identify the optimal number of principal components. We'll plot the cumulative distribution of variance across several principal components, and identify the number of components that describe 85% of the variance in the data.\n",
    "\n",
    "Our data suggests 0 - 2 components describe the data, thus a total of 3 components will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our Data\n",
    "pca = PCA().fit(X_train_std)\n",
    "pca_variance_components = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "## Plot Parameters\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(pca_variance_components)\n",
    "plt.xlabel('Number of Components', fontsize = 24)\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.ylabel('Variance (%)', fontsize = 24) \n",
    "plt.title('Variance of Principle Components', fontsize = 24)\n",
    "\n",
    "### Color markers and annotate text\n",
    "s = ['o', 'o', 'o', 'o']\n",
    "col = ['blue','red','green', 'black']\n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = pca_variance_components\n",
    "\n",
    "for _s, c, _x, _y in zip(s, col, x, y):\n",
    "    plt.scatter(_x, _y, marker = _s, c = c)\n",
    "\n",
    "### Annotate text\n",
    "plt.text(-0.05, pca_variance_components[0] + 0.015,\n",
    "         str(round(pca_variance_components[0], 3)), size = 14, color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(0.95, pca_variance_components[1] + 0.008,\n",
    "         str(round(pca_variance_components[1], 3)), size = 14, color = 'red', weight = 'semibold')\n",
    "\n",
    "plt.text(1.95, pca_variance_components[2] + 0.008,\n",
    "         str(round(pca_variance_components[2], 3)), size = 14, color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(2.96, pca_variance_components[3] + 0.005,\n",
    "         str(round(pca_variance_components[3], 3)), size = 14, color = 'black', weight = 'semibold')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Components Analysis\n",
    "## Set PCA parameters\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "## Fit and transform\n",
    "principalComponents = pca.fit(X_train_std)\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "principalComponents_variance = principalComponents.explained_variance_ratio_\n",
    "\n",
    "## Plot the expected variance\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.set(style = \"white\", rc = {\"lines.linewidth\": 3})\n",
    "sns.barplot(x = np.array(features), y = principalComponents.explained_variance_ratio_, palette = \"bright\")\n",
    "plt.xlabel('PCA features', fontsize = 24)\n",
    "plt.ylabel('Variance %', fontsize = 24)\n",
    "plt.title('PCA Variance', fontsize = 24)\n",
    "plt.xticks(features, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "### Annotate text\n",
    "plt.text(-0.07, principalComponents_variance[0] + 0.004,\n",
    "         str(round(principalComponents_variance[0], 3)), size = 'large', color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(0.92, principalComponents_variance[1] + 0.004,\n",
    "         str(round(principalComponents_variance[1], 3)), size = 'large', color = 'orange', weight = 'semibold')\n",
    "\n",
    "plt.text(1.93, principalComponents_variance[2] + 0.004,\n",
    "         str(round(principalComponents_variance[2], 3)), size = 'large', color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.4,\n",
    "         '0.585 ', size = 'large', color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.38,\n",
    "         '0.253 ', size = 'large', color = 'orange', weight = 'semibold')\n",
    "\n",
    "plt.text(1.90, principalComponents_variance[2] + 0.36,\n",
    "         '+', size = 'large', color = 'black', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.36,\n",
    "         '0.120', size = 'large', color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(1.90, principalComponents_variance[2] + 0.35,\n",
    "         '-----------', size = 'large', color = 'black', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.33,\n",
    "         '0.958 ≥ 0.85 ', size = 'large', color = 'black', weight = 'semibold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we begin with clustering we want to transform the training data using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PCA number of components\n",
    "pca = PCA(n_components = 3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_Fit_Transform(df):\n",
    "    \"\"\"Function to fit and transform the dataframe and rename the PCA components\"\"\"\n",
    "    PCA_DF = pca.fit_transform(df)\n",
    "    PCA_DF = pd.DataFrame(PCA_DF)\n",
    "    \n",
    "    # Rename PCA column names\n",
    "    x = (np.array(PCA_DF.columns)+1)\n",
    "    columns = []\n",
    "    for i in x:\n",
    "        columns.append('PCA' + ' ' + str(i))\n",
    "    PCA_DF.columns = columns\n",
    "    \n",
    "    return PCA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_Transform(df):\n",
    "    \"\"\"Function to transform the dataframe and rename the PCA components\"\"\"\n",
    "    PCA_DF = pca.transform(df)\n",
    "    PCA_DF = pd.DataFrame(PCA_DF)\n",
    "    \n",
    "    # Rename PCA column names\n",
    "    x = (np.array(PCA_DF.columns)+1)\n",
    "    columns = []\n",
    "    for i in x:\n",
    "        columns.append('PCA' + ' ' + str(i))\n",
    "    PCA_DF.columns = columns\n",
    "    \n",
    "    return PCA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_train = pca_Fit_Transform(X_train_std)\n",
    "PCA_components_dev = pca_Transform(X_dev_std)\n",
    "PCA_components_test = pca_Transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN clustering is an algorithm, which groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points [2]. It also marks as outliers the points that are in low-density regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm\n",
    "1. Arbitrarily select a point P.\n",
    "2. Retrieve all points directly density-reachable from P with respect to ε.\n",
    "3. If P is a core point, a cluster is formed. Find recursively all its density connected points and assign them to the same cluster as P.\n",
    "4. If P is not a core point, DBSCAN iterates through the remaining unvisited points in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters\n",
    "\n",
    "In DBSCAN, we'll be examining two parameters:\n",
    "* minPoints: the minimum number of points to form a dense region.\n",
    "* eps: specifies how close points should be to each other to be considered a part of a cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For minPoints, we will use a guideline in the original DBSCAN paper [2].\n",
    "\n",
    "minPoints = 2*dimensions (number of features)\n",
    "\n",
    "minPoints = 2 * 3 = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eps, we will use a K-nearest-neighbors distance plot to identify the knee in the plot and select that distance [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = 6\n",
    "nbrs = NearestNeighbors(n_neighbors = ns).fit(PCA_components_train)\n",
    "distances, indices = nbrs.kneighbors(PCA_components_train)\n",
    "distanceDec = sorted(distances[:,ns-1], reverse = False)\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(list(range(1, 20772)),\n",
    "         distanceDec, '--')\n",
    "plt.xticks(range(0, 22000, 1000))\n",
    "plt.xlabel('Samples', fontsize = 24)\n",
    "plt.ylabel('6-NN Distance', fontsize = 24)\n",
    "plt.title('Estimating eps using KNN distance')\n",
    "\n",
    "# Annotate Graph\n",
    "plt.hlines(y = 1.1, color = 'red', linestyle = '--', xmin = 0, xmax = 21000)\n",
    "\n",
    "plt.text(8500, 1.5 ,'ε = 1.1', size = 14, color = 'red', weight = 'semibold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with configuring all parameters for DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on Training Data\n",
    "dbscan = DBSCAN( eps = 1.1, min_samples = 6).fit(PCA_components_train)\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette analysis studies how similar and dissimilar neighboring cluster centroids are. We select the point which is closest to +1. In our case, the parameters have a silhouette score of 0.7672."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = metrics.silhouette_score(PCA_components_train, dbscan.labels_)\n",
    "print('The Silhouette Score for the training set is ' + str(ss) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load in our function to plot our 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter3d_cluster(df, x, y, z, code, title):\n",
    "    scatter = px.scatter_3d(df, x=x, y=y, z=z, color = code,  \n",
    "                            color_discrete_sequence=px.colors.qualitative.Light24)\n",
    "    \n",
    "    scatter.update_layout(title = title, title_font = dict(size = 30),\n",
    "                          scene = dict(\n",
    "                              xaxis = dict(\n",
    "                                  backgroundcolor=\"rgb(200, 200, 230)\",\n",
    "                                  gridcolor=\"white\",\n",
    "                                  showbackground=True,\n",
    "                                  zerolinecolor=\"white\",\n",
    "                                  nticks=10, ticks='outside',\n",
    "                                  tick0=0, tickwidth = 4,\n",
    "                                  title_font = dict(size = 16)),\n",
    "                              yaxis = dict(\n",
    "                                  backgroundcolor=\"rgb(230, 200,230)\",\n",
    "                                  gridcolor=\"white\",\n",
    "                                  showbackground=True,\n",
    "                                  zerolinecolor=\"white\",\n",
    "                                  nticks=10, ticks='outside',\n",
    "                                  tick0=0, tickwidth = 4,\n",
    "                                  title_font = dict(size = 16)),\n",
    "                              zaxis = dict(\n",
    "                                  backgroundcolor=\"rgb(230, 230,200)\",\n",
    "                                  gridcolor=\"white\",\n",
    "                                  showbackground=True,\n",
    "                                  zerolinecolor=\"white\",\n",
    "                                  nticks=10, ticks='outside',\n",
    "                                  tick0=0, tickwidth = 4,\n",
    "                                  title_font = dict(size = 16),\n",
    "                              ),\n",
    "                          ),\n",
    "                          width = 700\n",
    "                         )\n",
    "    return scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, plot the data and let's label the data with our assumptions on how hackers are perceived.\n",
    "\n",
    "Hackers tend to have high Kill-Death Ratios, Headshot-Kill Ratios, Top 10 Ratios, and Win Ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## 3D Plot of Training Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_train['Cluster'] = pd.Series(labels, index = PCA_components_train.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {0: \"Humans\", -1: 'Hackers'}\n",
    "PCA_components_train['Cluster_Labels'] = PCA_components_train['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_train , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_train['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 20706 humans and 65 hackers. Compared to only DBSCAN, PCA + DBSCAN has resulted in less outliers (hackers) detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_components_train.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_predict(model, X):\n",
    "    \"Predict function created for DBSCAN\"\n",
    "    nr_samples = X.shape[0]\n",
    "\n",
    "    y_new = np.ones(shape=nr_samples, dtype=int) * -1\n",
    "\n",
    "    for i in range(nr_samples):\n",
    "        diff = model.components_ - X.iloc[i, :].values  # NumPy broadcasting\n",
    "\n",
    "        dist = np.linalg.norm(diff, axis=1)  # Euclidean distance\n",
    "\n",
    "        shortest_dist_idx = np.argmin(dist)\n",
    "\n",
    "        if dist[shortest_dist_idx] < model.eps:\n",
    "            y_new[i] = model.labels_[model.core_sample_indices_[shortest_dist_idx]]\n",
    "\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_labels = dbscan_predict(dbscan, PCA_components_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3D Plot of deving Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_dev['Cluster'] = pd.Series(predict_labels, index = PCA_components_dev.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {0: \"Humans\", -1: 'Hackers'}\n",
    "PCA_components_dev['Cluster_Labels'] = PCA_components_dev['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_dev , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 7093 humans and 28 hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_components_dev.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN on test data\n",
    "predict_labels = dbscan_predict(dbscan, PCA_components_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3D Plot of testing Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_test['Cluster'] = pd.Series(predict_labels, index = PCA_components_test.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {0: \"Humans\", -1: 'Hackers'}\n",
    "PCA_components_test['Cluster_Labels'] = PCA_components_test['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_test , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 1773 humans and 8 hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_components_test.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "* Without external labels, we cannot verify the accuracy of these clusters. But we can make an educated guess on what these clusters are by using domain experience and advice from experts playing the game.\n",
    "* Treating this problem as an outlier detection (anomaly detection) problem has yielded promising results.\n",
    "    * With the assumption that the number of hackers in our population is low, we can treat them as outliers.\n",
    "* However, most of the outliers in our models can be misclassifying actual humans, so further tuning needs to be done to reduce that misclassification rate.\n",
    "* Look into alternative anomaly detection algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://www.cs.cmu.edu/~elaw/papers/pca.pdf\n",
    "\n",
    "[2] Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. ACM Transactions on Database Systems (TODS), 42(3), 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
