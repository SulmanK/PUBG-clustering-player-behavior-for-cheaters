{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PUBG_Logo](assets/PUBG_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "* Employ Prinicpal Components Analyis using selected features on dataset.\n",
    "* Employ K-means clustering algorithm on dataset.\n",
    "* Discuss pertinent results.\n",
    "\n",
    "## Background Information\n",
    "* Playerunknown's Battleground (PUBG) is a video game, which set the standard for preceding games in the Battle Royale Genre. The main goal is to SURVIVE at all costs.\n",
    "\n",
    "## Process:\n",
    "* Exploratory Data Analysis conducted utilizing various python packages (Numpy, Matplotlib, Pandas, and Plotly).\n",
    "* Principal Components Analysis (Sci-Kit Learn)\n",
    "* K-means clustering algorithm (Sci-Kit Learn)\n",
    "\n",
    "\n",
    "## Table of Contents:\n",
    "* Part I: Exploratory Data Analysis\n",
    "    * EDA\n",
    "* Part II: PCA / K-means Clustering\n",
    "    * Two Clusters\n",
    "    * Four Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing / Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by reading in the CSV file containing the data, and examining the data contents such as the number of features and rows. It seems there are 152 column entries (features) and 87898 row entries (number of samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- Pandas Dataframe\n",
    "## Read in CSV\n",
    "orig = pd.read_csv('data/PUBG_Player_Statistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us remove and combine features, which do not pertain to our goal of clustering solo player behavior. \n",
    "\n",
    "Remove:\n",
    "* player_name\n",
    "* tracker_id\n",
    "* duo\n",
    "* squad\n",
    "\n",
    "Add:\n",
    "* Total Distance\n",
    "\n",
    "This can be achieved by removing all columns after the 52nd. Also, create a new feature that combines the walking and riding distance.\n",
    "\n",
    "Also, we will reduce the variance in the data by removing players with less than the mean number of rounds in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Preprocessing\n",
    "## Create a copy of the dataframe\n",
    "df = orig.copy()\n",
    "cols = np.arange(52, 152, 1)\n",
    "\n",
    "# Drop entries if they have null values\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "## Drop columns after the 52nd index\n",
    "df.drop(df.columns[cols], axis = 1, inplace = True)\n",
    "\n",
    "## Drop player_name and tracker id\n",
    "df.drop(df.columns[[0, 1]], axis = 1, inplace = True)\n",
    "\n",
    "## Drop Knockout and Revives\n",
    "df.drop(df.columns[[49]], axis = 1, inplace = True)\n",
    "df.drop(columns = ['solo_Revives'], inplace = True)\n",
    "\n",
    "## Drop the string solo from all strings\n",
    "df.rename(columns = lambda x: x.lstrip('solo_').rstrip(''), inplace = True)\n",
    "\n",
    "## Combine a few columns \n",
    "df['TotalDistance'] = df['WalkDistance'] + df['RideDistance']\n",
    "df['AvgTotalDistance'] = df['AvgWalkDistance'] + df['AvgRideDistance']\n",
    "\n",
    "# Remove Outliers\n",
    "df = df.drop(df[df['RoundsPlayed'] < df['RoundsPlayed'].mean()].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into three sets: train, dev, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test set using Sci-Kit Learn\n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 10)\n",
    "dev, test = train_test_split(test, test_size = 0.2, random_state = 10)\n",
    "data = train\n",
    "\n",
    "print(\"The number of training samples is\", len(train))\n",
    "print(\"The number of development samples is\", len(dev))\n",
    "print(\"The number of testing samples is\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important we go through the final output to make sure that are data preprocessing is complete. And it looks great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_columns', 52):\n",
    "    print(data.describe(include = 'all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - PCA / Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure\n",
    "* PCA\n",
    "* K-means\n",
    "    * Two Clusters\n",
    "    * Four Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected the following features because of experts and my domain experience playing PUBG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select four features\n",
    "train_data = train.loc[:,['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]\n",
    "dev_data = dev.loc[:, ['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]\n",
    "test_data = test.loc[:, ['WinRatio', 'KillDeathRatio', \"HeadshotKillRatio\", \"Top10Ratio\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is utilized to make sure all features are normalized and have similar orders of magnitude. This is important because our clustering algorithms look into calculating the distance between points. In our case, we employed a zero-mean and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data (Normaliz)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(train_data)\n",
    "X_dev_std = scaler.transform(dev_data)\n",
    "X_test_std = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Principal Component Analysis is a dimensional reduction technique that preserves the essence of the original data [1]. Having multi-dimensional data (many features), it is very important in identifying which features are important. PCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along with them.\n",
    "\n",
    "\n",
    "#### Algorithm \n",
    "1. Calculate the covariance matrix X of data points.\n",
    "2. Calculate eigenvectors and corresponding eigen values.\n",
    "3. Sort the eigenvectors according to their eigen values in decreasing order.\n",
    "4. Choose first k eigen vectors and that will be the new k dimensions.\n",
    "5. Transform the original n dimensional data points into k dimensions.\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "PCA has one parameter:\n",
    "* Number of components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Number of Components\n",
    "\n",
    "Now, we'll want to identify the optimal number of principal components. We'll plot the cumulative distribution of variance across several principal components, and identify the number of components that describe 85% of the variance in the data.\n",
    "\n",
    "Our data suggests 0 - 2 components describe the data, thus a total of 3 components will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fitting the PCA algorithm with our Data\n",
    "pca = PCA().fit(X_train_std)\n",
    "pca_variance_components = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "## Plot Parameters\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(pca_variance_components)\n",
    "plt.xlabel('Number of Components', fontsize = 24)\n",
    "plt.xticks([0, 1, 2, 3])\n",
    "plt.ylabel('Variance (%)', fontsize = 24) \n",
    "plt.title('Variance of Principle Components', fontsize = 24)\n",
    "\n",
    "### Color markers and annotate text\n",
    "s = ['o', 'o', 'o', 'o']\n",
    "col = ['blue','red','green', 'black']\n",
    "x = np.array([0, 1, 2, 3])\n",
    "y = pca_variance_components\n",
    "\n",
    "for _s, c, _x, _y in zip(s, col, x, y):\n",
    "    plt.scatter(_x, _y, marker = _s, c = c)\n",
    "\n",
    "### Annotate text\n",
    "plt.text(-0.05, pca_variance_components[0] + 0.015,\n",
    "         str(round(pca_variance_components[0], 3)), size = 14, color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(0.95, pca_variance_components[1] + 0.008,\n",
    "         str(round(pca_variance_components[1], 3)), size = 14, color = 'red', weight = 'semibold')\n",
    "\n",
    "plt.text(1.95, pca_variance_components[2] + 0.008,\n",
    "         str(round(pca_variance_components[2], 3)), size = 14, color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(2.96, pca_variance_components[3] + 0.005,\n",
    "         str(round(pca_variance_components[3], 3)), size = 14, color = 'black', weight = 'semibold')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Components Analysis\n",
    "## Set PCA parameters\n",
    "pca = PCA(n_components = 3)\n",
    "\n",
    "## Fit and transform\n",
    "principalComponents = pca.fit(X_train_std)\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "principalComponents_variance = principalComponents.explained_variance_ratio_\n",
    "\n",
    "## Plot the expected variance\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.set(style = \"white\", rc = {\"lines.linewidth\": 3})\n",
    "sns.barplot(x = np.array(features), y = principalComponents.explained_variance_ratio_, palette = \"bright\")\n",
    "plt.xlabel('PCA features', fontsize = 24)\n",
    "plt.ylabel('Variance %', fontsize = 24)\n",
    "plt.title('PCA Variance', fontsize = 24)\n",
    "plt.xticks(features, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "### Annotate text\n",
    "plt.text(-0.07, principalComponents_variance[0] + 0.004,\n",
    "         str(round(principalComponents_variance[0], 3)), size = 'large', color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(0.92, principalComponents_variance[1] + 0.004,\n",
    "         str(round(principalComponents_variance[1], 3)), size = 'large', color = 'orange', weight = 'semibold')\n",
    "\n",
    "plt.text(1.93, principalComponents_variance[2] + 0.004,\n",
    "         str(round(principalComponents_variance[2], 3)), size = 'large', color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.4,\n",
    "         '0.585 ', size = 'large', color = 'blue', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.38,\n",
    "         '0.253 ', size = 'large', color = 'orange', weight = 'semibold')\n",
    "\n",
    "plt.text(1.90, principalComponents_variance[2] + 0.36,\n",
    "         '+', size = 'large', color = 'black', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.36,\n",
    "         '0.120', size = 'large', color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(1.90, principalComponents_variance[2] + 0.35,\n",
    "         '-----------', size = 'large', color = 'black', weight = 'semibold')\n",
    "\n",
    "plt.text(1.945, principalComponents_variance[2] + 0.33,\n",
    "         '0.958 ≥ 0.85 ', size = 'large', color = 'black', weight = 'semibold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we begin with clustering we want to transform the training data using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PCA number of components\n",
    "pca = PCA(n_components = 3, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_Fit_Transform(df):\n",
    "    \"\"\"Function to fit and transform the dataframe and rename the PCA components\"\"\"\n",
    "    PCA_DF = pca.fit_transform(df)\n",
    "    PCA_DF = pd.DataFrame(PCA_DF)\n",
    "    \n",
    "    # Rename PCA column names\n",
    "    x = (np.array(PCA_DF.columns)+1)\n",
    "    columns = []\n",
    "    for i in x:\n",
    "        columns.append('PCA' + ' ' + str(i))\n",
    "    PCA_DF.columns = columns\n",
    "    \n",
    "    return PCA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_Transform(df):\n",
    "    \"\"\"Function to transform the dataframe and rename the PCA components\"\"\"\n",
    "    PCA_DF = pca.transform(df)\n",
    "    PCA_DF = pd.DataFrame(PCA_DF)\n",
    "    \n",
    "    # Rename PCA column names\n",
    "    x = (np.array(PCA_DF.columns)+1)\n",
    "    columns = []\n",
    "    for i in x:\n",
    "        columns.append('PCA' + ' ' + str(i))\n",
    "    PCA_DF.columns = columns\n",
    "    \n",
    "    return PCA_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_train = pca_Fit_Transform(X_train_std)\n",
    "PCA_components_dev = pca_Transform(X_dev_std)\n",
    "PCA_components_test = pca_Transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "\n",
    "K-means clustering is an algorithm to classify or to group objects based on attributes/features into K number of groups [2]. The grouping is done by minimizing the sum of square of distances between data and the corresponding cluster centroid.\n",
    "Algorithm:\n",
    "\n",
    "1. Select K points randomly from the dataset as the centroids of the clusters.\n",
    "2. Assign data points to centroids closest to it.\n",
    "3. Recompute the centroid so that it is closest to all the data points allocated to that cluster.\n",
    "4. Repeat step 2 and 3 until the algorithm converges.\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "In K-means clustering, we'll be examining two parameters:\n",
    "\n",
    "* How clusters are initialized\n",
    "* The number of clusters\n",
    "\n",
    "##### Initialization\n",
    "\n",
    "Standard k-means clustering has a challenge initializing the cluster centroids. If a wrong cluster initialization is set, the clusters will be wrong. We'll be using the K-means +++ initialization to solve this issue by first initializing the cluster centroids before following the standard k-means clustering algorithm.\n",
    "\n",
    "1. The first cluster is chosen uniformly at random from the data points that we want to cluster. This is similar to what we do in K-Means, but instead of randomly picking all the centroids, we just pick one centroid here\n",
    "2. Compute the distance (D(x)) of each data point (x) from the cluster center that has already been chosen\n",
    "3. Choose the new cluster center from the data points with the probability of x being proportional to (D(x))2\n",
    "4. Repeat steps 2 and 3 until k clusters have been chosen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Optimal Number of Clusters\n",
    "Now, we'll want to identify the optimal number of clusters [3]. We'll use the inertia between clusters and silhouette analysis as our internal scoring metrics because we do not have access to correctly labeled data.\n",
    "\n",
    "\n",
    "Inertia is the within-cluster sum of squares. It calculates the variance of points in each cluster.\n",
    "\n",
    "Silhouette Analysis measures the similarity of points in a cluster and the dissimilarity of points in neighboring clusters.\n",
    "\n",
    "For our problem, SA provides a better solution because we are not looking for how points are similar in each cluster, but how they are separated.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "We plot the number of clusters vs the inertia, to identify the optimal number of clusters by selecting the number that is the elbow-point in the graph, or the point in which the graph doesn't have a steep slope.\n",
    "\n",
    "The elbow point is 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of clusters from 1 to 10\n",
    "ks = range(1, 10)\n",
    "\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters = k, init = 'k-means++', random_state = 10)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components_train)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    print('Inertia for %i Clusters: %0.4f' % (k, model.inertia_))\n",
    "\n",
    "# Plot parameters\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(ks, inertias, '-o', color = 'black')\n",
    "plt.plot(4, inertias[3], '-o', color = 'red', markersize = 12)\n",
    "plt.xlabel('Number of clusters', fontsize = 24)\n",
    "plt.ylabel('Inertia', fontsize = 24)\n",
    "plt.title('Optimal number of clusters (Inertia)', fontsize = 24)\n",
    "plt.xticks(ks, fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette analysis studies how similar and dissimilar neighboring cluster centroids are. We select the point which is closest to +1. In our case, two clusters have the greatest value following with four clusters.\n",
    "\n",
    "Being able to distinguish between a player and a hacker by having two clusters is the perfect solution. But from my experience, I feel there would be more segmentation than a cheater and a player such as\n",
    "\n",
    "* Beginner\n",
    "* Experienced\n",
    "* Professional\n",
    "* Hacker\n",
    "\n",
    "Nevertheless, we will explore all possible outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "ks = range(2, 10)\n",
    "score = []\n",
    "\n",
    "# Silhouette Method\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k, init = 'k-means++', random_state = 10).fit(PCA_components_train)\n",
    "    ss = metrics.silhouette_score(PCA_components_train, kmeans.labels_, sample_size = 10000)\n",
    "    score.append(ss)\n",
    "    print('Silhouette Score for %i Clusters: %0.4f' % (k, ss))\n",
    "\n",
    "# Plot Parameters\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(ks, score, '-o', color = 'blue')\n",
    "plt.xlabel(\"Number of clusters\", fontsize = 24)\n",
    "plt.ylabel(\"Silhouette score\", fontsize = 24)\n",
    "plt.title('Optinimal number of clusters (Silhouette)', fontsize = 24)\n",
    "\n",
    "## Different markers for first three points\n",
    "s = ['D', 'D', 'D' ]\n",
    "col = ['red','green','orange' ]\n",
    "x = np.array([2, 3, 4, 3])\n",
    "y = score[0:3]\n",
    "for _s, c, _x, _y in zip(s, col, x, y):\n",
    "    plt.scatter(_x, _y, marker = _s, c = c, s = 100)\n",
    "\n",
    "\n",
    "## Annotate text\n",
    "plt.text(1.90, score[0] + 0.005, str(round(score[0], 3)),\n",
    "         size = 14, color = 'red', weight = 'semibold')\n",
    "\n",
    "plt.text(2.97, score[1] + 0.005, str(round(score[1], 3)),\n",
    "         size = 14, color = 'green', weight = 'semibold')\n",
    "\n",
    "plt.text(3.90, score[2] + 0.005, str(round(score[2], 3)),\n",
    "         size = 14, color = 'orange', weight = 'semibold')\n",
    "\n",
    "plt.xticks(ks, fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with configuring all parameters for k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means train\n",
    "number_cluster = 2\n",
    "kmeans = KMeans(n_clusters = number_cluster, init = 'k-means++', random_state = 10).fit(PCA_components_train)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load in our function to plot our 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter3d_cluster(df, x, y, z, code, title):\n",
    "    scatter  =  px.scatter_3d(df, x = x, y = y, z = z, color = code,  \n",
    "                            color_discrete_sequence = px.colors.qualitative.Light24)\n",
    "    \n",
    "    scatter.update_layout(title = title, title_font = dict(size  =  30),\n",
    "                          scene = dict(\n",
    "                              xaxis = dict(\n",
    "                                  backgroundcolor = \"rgb(200, 200, 230)\",\n",
    "                                  gridcolor = \"white\",\n",
    "                                  showbackground = True,\n",
    "                                  zerolinecolor = \"white\",\n",
    "                                  nticks = 10, ticks = 'outside',\n",
    "                                  tick0 = 0, tickwidth  =  4,\n",
    "                                  title_font  =  dict(size  =  16)),\n",
    "                              yaxis  =  dict(\n",
    "                                  backgroundcolor = \"rgb(230, 200,230)\",\n",
    "                                  gridcolor = \"white\",\n",
    "                                  showbackground = True,\n",
    "                                  zerolinecolor = \"white\",\n",
    "                                  nticks = 10, ticks = 'outside',\n",
    "                                  tick0 = 0, tickwidth  =  4,\n",
    "                                  title_font  =  dict(size  =  16)),\n",
    "                              zaxis  =  dict(\n",
    "                                  backgroundcolor = \"rgb(230, 230,200)\",\n",
    "                                  gridcolor = \"white\",\n",
    "                                  showbackground = True,\n",
    "                                  zerolinecolor = \"white\",\n",
    "                                  nticks = 10, ticks = 'outside',\n",
    "                                  tick0 = 0, tickwidth  =  4,\n",
    "                                  title_font  =  dict(size  =  16),\n",
    "                              ),\n",
    "                          ),\n",
    "                          width  =  700\n",
    "                         )\n",
    "    return scatter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, plot the data and let's label the data with our assumptions on how hackers are perceived.\n",
    "\n",
    "Hackers tend to have higher combat statistics than the rest of the playerbase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 3D Plot of Training Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_train['Cluster'] = pd.Series(labels, index = PCA_components_train.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {1: \"Humans\", 0: 'Hackers'}\n",
    "PCA_components_train['Cluster_Labels'] = PCA_components_train['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_train , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 17225 humans and 3546 hackers. This seems to be an enormous amount of hackers, and I do not trust the results, maybe four clusters will provide a better result. Nevertheless, let's continue with predicting on the dev and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_train.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's take a look at the scatter matrix of principal components. From our results, the first three components display distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattermatrix\n",
    "fig = px.scatter_matrix(PCA_components_train, \n",
    "                        dimensions = [\"PCA 1\", \"PCA 2\", \"PCA 3\"], \n",
    "                        color = 'Cluster_Labels', color_discrete_sequence = px.colors.qualitative.Light24)\n",
    "\n",
    "## Remove diagonal plots\n",
    "fig.update_traces(diagonal_visible = False)\n",
    "\n",
    "## Plot Parameters\n",
    "fig.update_layout(\n",
    "    title = 'Scattermatrix of PCA components with Two Clusters',\n",
    "    title_font = dict(size = 24),\n",
    "    width = 1000,\n",
    "    height = 1000,\n",
    "    scene = dict(\n",
    "        xaxis = dict(title_font = dict(size = 18)),\n",
    "        yaxis = dict(title_font = dict(size = 18)),\n",
    "    )   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering of Testing Data\n",
    "predict_labels = kmeans.predict(PCA_components_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot of Training Data\n",
    "## Create and modify dataframe for the cluster column\n",
    "PCA_components_dev['Cluster'] = pd.Series(predict_labels, index = PCA_components_dev.index)\n",
    "\n",
    "## Rename Cluster label names from k-means\n",
    "cluster_label_names = {1: \"Humans\", 0: 'Hackers'}\n",
    "PCA_components_dev['Cluster_Labels'] = PCA_components_dev['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "## Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_dev , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 5899 humans and 1222 hackers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_dev.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering of Testing Data\n",
    "predict_labels = kmeans.predict(PCA_components_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot of Training Data\n",
    "#@ Create and modify dataframe for the cluster column\n",
    "PCA_components_test['Cluster'] = pd.Series(predict_labels, index = PCA_components_test.index)\n",
    "\n",
    "#@ Rename Cluster label names from k-means\n",
    "cluster_label_names = {1: \"Humans\", 0: 'Hackers'}\n",
    "PCA_components_test['Cluster_Labels'] = PCA_components_test['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "#@ Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_test , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 1485 humans and 296 hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_components_test.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Four Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with performing PCA on dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PCA number of components\n",
    "pca = PCA(n_components = 3, random_state = 10)\n",
    "\n",
    "# Set PCA_dataframes\n",
    "PCA_components_train = pca_Fit_Transform(X_train_std)\n",
    "PCA_components_dev = pca_Transform(X_dev_std)\n",
    "PCA_components_test = pca_Transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure all parameters for k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means train\n",
    "number_cluster = 4\n",
    "kmeans = KMeans(n_clusters = number_cluster, init = 'k-means++', random_state = 10).fit(PCA_components_train)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 3D Plot of Training Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_train['Cluster'] = pd.Series(labels, index = PCA_components_train.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {2: \"Experienced\", 0: \"Beginner\", 1: \"Professional\", 3: \"Hacker\"}\n",
    "PCA_components_train['Cluster_Labels'] = PCA_components_train['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_train , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 8979 beginners, 6666 experienced, 4294 professionals, and 832 hackers. This seems more reasonable in how many hackers were selected, as I'd expect hackers to take up a small number of the population roughly 0 to 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_train.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's take a look at the scatter matrix of principal components. From our results, the first three components display distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scattermatrix\n",
    "fig = px.scatter_matrix(PCA_components_train, \n",
    "                        dimensions = [\"PCA 1\", \"PCA 2\", \"PCA 3\"], \n",
    "                        color = 'Cluster_Labels', color_discrete_sequence = px.colors.qualitative.Light24)\n",
    "## Remove diagonal plots\n",
    "fig.update_traces(diagonal_visible = False)\n",
    "\n",
    "## Plot Parameters\n",
    "fig.update_layout(\n",
    "    title = 'Scattermatrix of PCA components with Four Clusters',\n",
    "    title_font = dict(size = 24),\n",
    "    width = 1000,\n",
    "    height = 1000,\n",
    "    scene = dict(\n",
    "        xaxis = dict(title_font = dict(size = 18)),\n",
    "        yaxis = dict(title_font = dict(size = 18)),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the dev set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # K means clustering of Testing Data\n",
    "predict_labels = kmeans.predict(PCA_components_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3D Plot of Training Data\n",
    "# Create and modify dataframe for the cluster column\n",
    "PCA_components_dev['Cluster'] = pd.Series(predict_labels, index = PCA_components_dev.index)\n",
    "\n",
    "#Rename Cluster label names from k-means\n",
    "cluster_label_names = {2: \"Experienced\", 0: \"Beginner\", 1: \"Professional\", 3: \"Hacker\"}\n",
    "PCA_components_dev['Cluster_Labels'] = PCA_components_dev['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "# Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_dev , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 3112 beginners, 2258 experienced, 1439 professionals, and 312 hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components_dev.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predicting on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with predicting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering of Testing Data\n",
    "predict_labels = kmeans.predict(PCA_components_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot of Training Data\n",
    "## Create and modify dataframe for the cluster column\n",
    "PCA_components_test['Cluster'] = pd.Series(predict_labels, index = PCA_components_test.index)\n",
    "\n",
    "## Rename Cluster label names from k-means\n",
    "cluster_label_names = {2: \"Experienced\", 0: \"Beginner\", 1: \"Professional\", 3: \"Hacker\"}\n",
    "PCA_components_test['Cluster_Labels'] = PCA_components_test['Cluster'].map(cluster_label_names)  \n",
    "\n",
    "## Plots of PCA 1, PCA 2, PCA 3\n",
    "scatter3d_cluster(df = PCA_components_test , x = 'PCA 1',\n",
    "                  y = 'PCA 2', z = 'PCA 3', code = 'Cluster_Labels', title = 'PCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we observed roughly 756 beginners, 573 experienced, 381 professionals, and 71 hackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_components_test.groupby('Cluster_Labels').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "* Without external labels, we cannot verify the accuracy of these clusters. But we can make an educated guess on what these clusters are by using domain experience and advice from experts playing the game.\n",
    "* Also, it's difficult to interpret the results of the PCA components because we do not know what features are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://www.cs.cmu.edu/~elaw/papers/pca.pdf\n",
    "\n",
    "[2] \"Survey Report on K-Means Clustering Algorithm\", International Journal of Modern Trends in Engineering & Research, vol. 4, no. 4, pp. 218-221, 2017. Available: 10.21884/ijmter.2017.4143.lgjzd.\n",
    "\n",
    "[3] https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
